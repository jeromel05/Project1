{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from UtilityFunctions import *\n",
    "from datapreprocessing import *\n",
    "from implementations import *\n",
    "from patternsmissingvalues import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape,tX.shape,ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tX[:,0].shape,np.std(tX[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tX[:,0],y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne sont pas des outliers, mais contiennent juste des missing values à -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#corrcoef prod des vals normalisées à partir de la mat de correlation\n",
    "cov = np.cov(tX.T)\n",
    "#print(cov.shape, np.max(cov))\n",
    "corr = np.corrcoef((tX.T))\n",
    "#print(corr.shape, (corr))\n",
    "plt.imshow(corr, cmap='hot')\n",
    "res = np.where(corr > 0.95) #arbitrary threshold for strong correlation\n",
    "listOfCoordinates= list(zip(res[0], res[1]))\n",
    "listOfCoordinates = [cord for cord in listOfCoordinates if cord[0] != cord[1]] #remove diagonal\n",
    "listOfCoordinates = {tuple(sorted(t)): t for t in listOfCoordinates}#remove commutative elements\n",
    "for cord in listOfCoordinates:\n",
    "        print(cord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify strong correlations between the features listed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca(tX)\n",
    "plt.scatter(X_pca[:,0],y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the first PCA component against the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm # To colour dots of scatter plot\n",
    "colors = cm.rainbow(y)\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1], marker=\".\", color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the first PCA component against the second PCA component. The dots are color labeled according to their output value y. Unfortunately, we notice that the clusters identified in the plot do not discriminate between the 2 output categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values positions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX2 = tX.copy()\n",
    "res = []\n",
    "for el in tX2.T:\n",
    "    res.append(el[el > -999])\n",
    "\n",
    "mu = [np.mean(el) for el in res]\n",
    "sigma = [np.std(el) for el in res]\n",
    "\n",
    "for col, mu1, sigma1 in zip(tX2.T,mu,sigma):\n",
    "    col[col == -999] = np.random.normal(mu1, sigma1, np.sum([col == -999][0]))\n",
    "    \n",
    "print(np.sum(tX==-999), np.sum(tX2==-999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions of the variables (w/o missing values) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val = np.zeros(tX.shape)\n",
    "missing_val[tX==-999] = 1\n",
    "total_cols = np.sum(missing_val, axis=0)/y.shape\n",
    "total_rows = np.sum(missing_val, axis=1)/np.shape(tX)[1]\n",
    "tX_reduced = tX[: ,total_cols < 0.5] #select only cols where less than 50% val missing and < 30% for rows\n",
    "tX_reduced = tX_reduced[total_rows<0.3, :] #and < 30% for rows\n",
    "y_reduced = y[total_rows<0.3]\n",
    "\n",
    "fig,ax = plt.subplots(5, 6, sharex='col')\n",
    "for i in range(tX.shape[1]):\n",
    "    tXcol = [el for el in tX[:,i] if el > -999]  #keep only values in the normal range for the plot\n",
    "    ax[i%5,i%6].hist(tXcol, bins=40, log=True)\n",
    "fig.set_figheight(150)\n",
    "fig.set_figwidth(150)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global data preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(tX)\n",
    "tX = adding_offset(x)\n",
    "xtrain,ytrain,xtest,ytest=split_data(tX,y,0.9)\n",
    "#pX=build_poly(tX, 5) #-- marche pas --> parce que la fonction est prévu que pour le cas 1D\n",
    "pX= add_higher_degree_terms(tX,3)\n",
    "xtrainpol,ytrainpol,xtestpol,ytestpol=split_data(pX, y, 0.9) #on polynomial version,with ration= 0.9 and default seed=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares gradient descent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(np.shape(tX)[1])\n",
    "gamma = 0.0825\n",
    "max_iters = 1000\n",
    "final_w_gd, final_loss_gd = least_squares_GD(y, tX, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares stochastic gradient descent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.005\n",
    "max_iters = 500\n",
    "final_w_sgd, final_loss_sgd = least_squares_SGD(y, tX, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares with normal equation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_ne,loss_ne= least_squares(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on polynomial version of the data set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ne_pol, loss_ne_pol = least_squares(y, pX)\n",
    "# tester ici quel degré est le mieux pour pX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and with split of test and train data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ne_pol_train, loss_ne_pol_train=least_squares(ytrainpol, xtrainpol)\n",
    "rmse_ne_pol_train=rmse(loss_ne_pol_train)\n",
    "loss_ne_pol_test= compute_loss(ytestpol, xtestpol, w_ne_pol_train)\n",
    "rmse_ne_pol_test=rmse(loss_ne_pol_test)\n",
    "print(\" Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(tr=rmse_ne_pol_train, te=rmse_ne_pol_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rmse_rr = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "        w_rr,loss_rr = ridge_regression(y, tX, lambda_)\n",
    "        rmse_rr.append(rmse(loss_rr))\n",
    "\n",
    "plot_implementation(rmse_rr, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on polynomial version of the data set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rmse_rr_pol = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "        w_rr_pol,loss_rr_pol = ridge_regression(y, pX, lambda_)\n",
    "        rmse_rr_pol.append(rmse(loss_rr_pol))\n",
    "\n",
    "plot_implementation(rmse_rr_pol, lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and with split of test and train data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rmse_rr_pol_train = []\n",
    "rmse_rr_pol_test = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "        w_rr_pol,loss_rr_pol_train = ridge_regression(ytrainpol,xtrainpol , lambda_)\n",
    "        rmse_rr_pol_train.append(rmse(loss_rr_pol_train))\n",
    "        rmse_rr_pol_test.append(rmse(compute_loss(ytestpol,xtestpol,w_rr_pol)))\n",
    "plot_train_test(rmse_rr_pol_train, rmse_rr_pol_test, lambdas)\n",
    "#JEROME: plot a l'air bizarre, exactement meme erreur pour test et train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example - base mode\n",
    "gamma=0.5\n",
    "ws_1,log_likelihoods_1 = logistic_regression(rescale_y(y), tX, np.random.rand(tX.shape[1])/1000000,10, gamma)\n",
    "plt.scatter(rescale_predictions(compute_p(ws_1[-1],tX))[1:1000],y[1:1000], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_2, log_likelihoods_2 = logistic_regression(rescale_y(y), tX, np.random.rand(tX.shape[1])/1000000,10,gamma)\n",
    "plt.scatter(rescale_predictions(compute_p(ws_2[-1],tX))[1:1000],y[1:1000], marker=\".\", color='b')\n",
    "np.linalg.norm(ws_1[-1]-ws_2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Adding w0 to the model\n",
    "\n",
    "tX_w0 = adding_offset(tX)\n",
    "\n",
    "print(tX_w0[0:5,0:5],tX_w0.shape)\n",
    "\n",
    "ws_3,log_likelihoods_3 = logistic_regression(rescale_y(y), tX_w0, np.random.rand(tX_w0.shape[1])/1000000,10)\n",
    "\n",
    "plt.scatter(rescale_predictions(compute_p(ws_3[-1],tX_w0))[1:1000],y[1:1000], marker=\".\", color='b')\n",
    "\n",
    "ws_4,log_likelihoods_4 = logistic_regression(rescale_y(y), tX_w0, np.random.rand(tX_w0.shape[1])/1000000,10)\n",
    "\n",
    "plt.scatter(rescale_predictions(compute_p(ws_4[-1],tX_w0))[1:1000],y[1:1000], marker=\".\", color='b')\n",
    "\n",
    "np.linalg.norm(ws_3[-1]-ws_4[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### replace -999 by average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_corr = set_missing_explanatory_vars_to_mean(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_corr[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_corr_pca = pca(tX_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.rainbow(y)\n",
    "plt.scatter(tX_corr_pca[:,0],tX_corr_pca[:,1], marker=\".\", color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = cm.rainbow(y[1:5000])\n",
    "plt.scatter(tX_corr_pca[1:5000,0],tX_corr_pca[1:5000,1], marker=\".\", color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find and delete outlier\n",
    "\n",
    "ind = np.arange(len(tX_corr_pca[:,0]))\n",
    "ind_outliers = ind[np.where(tX_corr_pca[:,0] < -50)] # find a suitable criterion, \n",
    "y_cleared = np.delete(y,ind_outliers)\n",
    "tX_corr_cleared = np.delete(tX_corr_pca,ind[np.where(tX_corr_pca[:,0] < -50)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_cleared.shape,tX_corr_cleared.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_corr_cleared_w0 = adding_offset(tX_corr_cleared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_5,log_likelihoods_5 = logistic_regression(rescale_y(y_cleared), tX_corr_cleared_w0, np.random.rand(tX_corr_cleared_w0.shape[1])/1000000,10,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rescale_predictions(compute_p(ws_5[-1],tX_corr_cleared_w0))[1:1000],y_cleared[1:1000], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_6, log_likelihoods_6 = logistic_regression(rescale_y(y_cleared), tX_corr_cleared_w0, np.random.rand(tX_corr_cleared_w0.shape[1])/1000000,10,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rescale_predictions(compute_p(ws_6[-1],tX_corr_cleared_w0))[1:1000],y_cleared[1:1000], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(ws_5[-1]-ws_6[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(ws_3[-1]-ws_5[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ws_3[-1],ws_5[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking into account patterns for the handling of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../data/test.csv/test.csv\"\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "DATA_TRAIN_PATH = \"../data/train.csv/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that in both test and train data the same columns are full of gaps\n",
    "tX_mv = sum(tX == -999) # Array with columnwise count of faulty measurements in training data\n",
    "tX_mv[tX_mv > 0] = 1 \n",
    "tX_test_mv = sum(tX_test == -999) # Array with columnwise count of faulty measurements in test data\n",
    "tX_test_mv[tX_test_mv > 0] = 1\n",
    "\n",
    "np.linalg.norm(tX_mv-tX_test_mv) # = 0 means both training and test data have values missing only in the excact same explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values don't appear to be random\n",
    "tX_mv = sum(tX == -999)\n",
    "tX_test_mv = sum(tX_test == -999)\n",
    "\n",
    "print(tX_mv,tX_test_mv) # Arrays with columnwise counts of faulty measurements in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(tX.shape[1])\n",
    "tX_col_mv = tX[:,ind[np.where(tX_mv > 0)]]\n",
    "test = np.zeros(tX_col_mv.shape)\n",
    "test[np.where(tX_col_mv == -999)] = 1\n",
    "print(test[1:10,:]) # Extracted coulums that contain -999s. '1' represents missing value, '0' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flip(np.power(10,np.arange(test.shape[1]),dtype=np.int64)) # Sleight of hand: matrix multiplication  with the following array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(test,np.flip(np.power(10,np.arange(test.shape[1]),dtype=np.int64))) # The pattern of missing columns is represented by an array of numbers representing all samples in the usual order. The numbers are binary code for the matrix of the extracted columns just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.dot(test,np.power(10,np.arange(test.shape[1]),dtype=np.int64))) # All the different configurations of the missing values. There are only six of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pattern of missing values and logistic regession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data...\n",
    "tX_split, ind_row_groups, groups_mv_num = split_data_according_to_pattern_of_missing_values(tX) # => regroupement of the steps performed above\n",
    "y_split = split_y_according_to_pattern_of_missing_values(y, ind_row_groups)\n",
    "\n",
    "'''import matplotlib.cm as cm\n",
    "\n",
    "for tX_group, y_group in zip(tX_split,y_split):\n",
    "    tX_group_pca = pca(tX_group)\n",
    "    colors = cm.rainbow(y_group)\n",
    "    plt.scatter(tX_group_pca[:,0],tX_group_pca[:,1], marker=\".\", color=colors)'''\n",
    "\n",
    "# ... and finding each subgroup's the optimal weights as well as predicting the y's of the training set using linear regression\n",
    "#     => condensed version is the function 'find_optimal_weights_pattern_mv'\n",
    "\n",
    "ws_groups = [] # list to stock weights of subgroups\n",
    "y_pred = np.zeros([len(y)]) # initialize array y_pred\n",
    "\n",
    "for tX_group, y_group, ind_row_group in zip(tX_split,y_split,ind_row_groups) :\n",
    "    #print(tX_group, y_group, ind_row_group)\n",
    "    \n",
    "    # All-zero columns appear in the reduced data sets. They interfere with the calculation of the jacobean matrix.\n",
    "    # The corresponding weights are set to zero and the other weights are determined using a further reduced dataset without those columns.\n",
    "    \n",
    "    ind_col_non_zero = np.arange(len(tX_group[0,:]))[sum(tX_group**2)>0]\n",
    "    ws_1,log_likelihoods_1 = logistic_regression(rescale_y(y_group), tX_group[:,ind_col_non_zero], np.random.rand(tX_group[:,ind_col_non_zero].shape[1])/1000000,10,gamma)\n",
    "    ws_2,log_likelihoods_2 = logistic_regression(rescale_y(y_group), tX_group[:,ind_col_non_zero], np.random.rand(tX_group[:,ind_col_non_zero].shape[1])/1000000,10,gamma)\n",
    "    assert np.allclose(ws_1[-1], ws_2[-1]) # The weights are calculated twice with different initial ws => asserting that they are the same.\n",
    "    w = np.zeros(len(tX_group[0,:]))\n",
    "    w[ind_col_non_zero] = ws_1[-1]\n",
    "    ws_groups.append(w)\n",
    "    y_pred[ind_row_group] = rescale_predictions(compute_p(w,tX_group))\n",
    "\n",
    "plt.scatter(y_pred[1:500],y[1:500], marker=\".\", color='b') # Plotting the predictions against the known values of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_split_test, ind_row_groups_test, groups_mv_num_test = split_data_according_to_pattern_of_missing_values(tX_test)\n",
    "\n",
    "\n",
    "''' Auxiliary function that calculates the predictions of each subgroup, joins them together in the right order\n",
    "    and rescales the predictions so that they lie between -1 and 1'''\n",
    "y_pred_test = predict_y_lr_weights_pattern_mv(ws_groups, tX_split_test,ind_row_groups_test,tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same with helper functions\n",
    "DATA_TRAIN_PATH = \"../data/train.csv/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data...\n",
    "tX_split, ind_row_groups, groups_mv_num = split_data_according_to_pattern_of_missing_values(tX)\n",
    "y_split = split_y_according_to_pattern_of_missing_values(y, ind_row_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms to the split data and calculating the optimal weights of each subgroup.\n",
    "ws_groups, y_pred = find_optimal_weights_pattern_mv(tX_split_test, ind_row_groups,y_split,tX.shape[0])\n",
    "plt.scatter(y_pred,y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred[ind_row_groups[4]][0:800],y[ind_row_groups[4]][0:800], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../data/test.csv/test.csv\"\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_split_test, ind_row_groups_test, groups_mv_num_test = split_data_according_to_pattern_of_missing_values(tX_test)\n",
    "y_pred_test = predict_y_lr_weights_pattern_mv(ws_groups, tX_split_test,ind_row_groups_test,tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tX = y_pred\n",
    "y_pred_test_tX = y_pred_test\n",
    "ws_groups_tX = ws_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "print(add_interaction_terms(x),add_square_terms(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add_higher_degree_terms(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best version so far: Pattern of missing values, interactions between variables and logistic regession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Squares of explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"../data/train.csv/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data...\n",
    "tX_split, ind_row_groups, groups_mv_num = split_data_according_to_pattern_of_missing_values(tX)\n",
    "y_split = split_y_according_to_pattern_of_missing_values(y, ind_row_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms to the split data and calculating the optimal weights of each subgroup.\n",
    "ws_groups, y_pred = find_optimal_weights_pattern_mv(add_exponential_terms_to_split_data(tX_split_test,2), ind_row_groups,y_split,tX.shape[0])\n",
    "plt.scatter(y_pred,y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred[ind_row_groups[4]][0:800],y[ind_row_groups[4]][0:800], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../data/test.csv/test.csv\"\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_split_test, ind_row_groups_test, groups_mv_num_test = split_data_according_to_pattern_of_missing_values(tX_test)\n",
    "y_pred_test = predict_y_lr_weights_pattern_mv(ws_groups, add_exponential_terms_to_split_data(tX_split_test,2),ind_row_groups_test,tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_squares = y_pred\n",
    "y_pred_test_squares = y_pred_test\n",
    "ws_groups_squares = ws_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred_test_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubic values of explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"../data/train.csv/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data...\n",
    "tX_split, ind_row_groups, groups_mv_num = split_data_according_to_pattern_of_missing_values(tX)\n",
    "y_split = split_y_according_to_pattern_of_missing_values(y, ind_row_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms to the split data and calculating the optimal weights of each subgroup.\n",
    "ws_groups, y_pred = find_optimal_weights_pattern_mv(add_exponential_terms_to_split_data(tX_split,3), ind_row_groups,y_split,tX.shape[0])\n",
    "plt.scatter(y_pred,y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred[ind_row_groups[4]][0:800],y[ind_row_groups[4]][0:800], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../data/test.csv/test.csv\"\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_split_test, ind_row_groups_test, groups_mv_num_test = split_data_according_to_pattern_of_missing_values(tX_test)\n",
    "y_pred_test = predict_y_lr_weights_pattern_mv(ws_groups, add_exponential_terms_to_split_data(tX_split_test,3),ind_row_groups_test,tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cubic = y_pred\n",
    "y_pred_test_cubic = y_pred_test\n",
    "ws_groups_cubic = ws_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_no_conv = np.array([1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_comb = y_pred_cubic\n",
    "for group in groups_no_conv:\n",
    "    y_pred_comb[ind_row_groups[group]] = y_pred_squares[ind_row_groups[group]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred_comb[ind_row_groups[1]][0:800],y[ind_row_groups[1]][0:800], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in groups_no_conv:\n",
    "    y_pred_test[ind_row_groups_test[group]] = y_pred_test_squares[ind_row_groups_test[group]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions beween all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"../data/train.csv/train.csv\" # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data...\n",
    "tX_split, ind_row_groups, groups_mv_num = split_data_according_to_pattern_of_missing_values(tX)\n",
    "y_split = split_y_according_to_pattern_of_missing_values(y, ind_row_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms to the split data and calculating the optimal weights of each subgroup.\n",
    "ws_groups, y_pred = find_optimal_weights_pattern_mv(add_interaction_terms_to_split_data(tX_split), ind_row_groups,y_split,tX.shape[0])\n",
    "plt.scatter(y_pred,y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction terms to the split data and calculating the optimal weights of each subgroup.\n",
    "ws_groups, y_pred = find_optimal_weights_pattern_mv(add_interaction_terms_to_split_data(tX_split), ind_row_groups,y_split,tX.shape[0])\n",
    "plt.scatter(y_pred,y, marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred[ind_row_groups[4]][0:800],y[ind_row_groups[4]][0:800], marker=\".\", color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"../data/test.csv/test.csv\"\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_split_test, ind_row_groups_test, groups_mv_num_test = split_data_according_to_pattern_of_missing_values(tX_test)\n",
    "y_pred_test = predict_y_lr_weights_pattern_mv(ws_groups, add_interaction_terms_to_split_data(tX_split_test),ind_row_groups_test,tX_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_interact = y_pred\n",
    "y_pred_test_interact = y_pred_test\n",
    "ws_groups_interact = ws_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standartization - if one uses square and cubic values they might be larger of smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "\n",
    "    centered_data = x - np.mean(x, axis=0)\n",
    "    std_data = centered_data / np.std(centered_data, axis=0)\n",
    "    \n",
    "    return std_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/predicted.csv' # TODO: fill in desired name of output file for submission\n",
    "tX_test, mean_x, std_x = standardize(tX_test)\n",
    "tX_test  = np.c_[np.ones(tX_test.shape[0]), tX_test]\n",
    "y_pred = predict_labels(w1, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementer 10% du train set comme test set"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
